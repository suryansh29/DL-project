{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryansh29/DL-project/blob/main/Notebook2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVahIS0z-EQE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jRZIgkHs_Wx"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSetW90l9xc_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, ClassLabel, concatenate_datasets\n",
        "import torch\n",
        "\n",
        "mnli = load_dataset(\"nyu-mll/multi_nli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1otq-MBh-RO3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, OPTForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, OPTForSequenceClassification, OPTForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "classification_model = OPTForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=50272)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=2)\n",
        "\n",
        "causal_model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "classification_model.score = causal_model.lm_head\n",
        "model = classification_model"
      ],
      "metadata": {
        "id": "e1fdNV4opYm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5eZ-tA2_P7A"
      },
      "outputs": [],
      "source": [
        "PATTERN=\"{text1} {text2} ?\"\n",
        "TARGET_PREFIX=\" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eINaG3QHdEw"
      },
      "outputs": [],
      "source": [
        "target_tokens_ids = tokenizer.convert_tokens_to_ids(['ĠYes', 'ĠNo'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bJiRpPgMe4g"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "mnli = mnli.filter(lambda example: example[\"label\"] != 1 and example[\"label\"] != -1)\n",
        "\n",
        "# change labels of contradiction examples from 2 to 1\n",
        "def change_label(example):\n",
        "    example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
        "    return example\n",
        "mnli = mnli.map(change_label)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CknLpuZpdvIV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhjxOy3dRZxX"
      },
      "outputs": [],
      "source": [
        "mnli\n",
        " # change features to reflect the new labels\n",
        "features = mnli[\"train\"].features.copy()\n",
        "\n",
        "# features[\"label\"] = ClassLabel(num_classes=len(tokenizer))\n",
        "features[\"label\"] = ClassLabel(num_classes=50272)\n",
        "\n",
        "mnli = mnli.cast(features)  # overwrite old features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCss0bY8Skf7"
      },
      "outputs": [],
      "source": [
        "def preprocess_function_without_context(examples):\n",
        "  pattern_examples = [\n",
        "          PATTERN.format(\n",
        "              text1=examples['premise'][idx],\n",
        "              text2=examples['hypothesis'][idx]) + TARGET_PREFIX\n",
        "          for idx in range(len(examples['premise']))\n",
        "      ]\n",
        "\n",
        "  args = (pattern_examples,)\n",
        "  result = tokenizer(*args, padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Get tokens\n",
        "  result[\"input_tokens\"] = [tokenizer.convert_ids_to_tokens(\n",
        "      ids) for ids in result[\"input_ids\"]]\n",
        "\n",
        "  # Decode input\n",
        "  result[\"input_text\"] = [tokenizer.decode(\n",
        "      ids) for ids in result[\"input_ids\"]]\n",
        "\n",
        "  result[\"label\"] = [target_tokens_ids[l] for l in examples[\"label\"]]\n",
        "\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdghx1ZAYQHt"
      },
      "outputs": [],
      "source": [
        "tokenized_mnli_without_context = mnli.map(preprocess_function_without_context, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NNaJptkUoeh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_few_shot_examples(examples, num_shots=3):\n",
        "  indices = np.random.choice(range(len(examples['premise'])), size=num_shots, replace=False)\n",
        "  separate_shots_by = \"\\n\\n\"\n",
        "  context=\"\"\n",
        "  for idx in indices:\n",
        "    formated_sample = PATTERN.format(\n",
        "        text1=examples['premise'][idx],\n",
        "        text2=examples['hypothesis'][idx]\n",
        "    )\n",
        "    verbalized_label = tokenizer.convert_ids_to_tokens([(examples['label'][idx])])[0]\n",
        "    context += f\"{formated_sample}{TARGET_PREFIX}{verbalized_label}{separate_shots_by}\"\n",
        "  return context\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_function_with_few_shot_context(examples):\n",
        "  examples[\"label\"] = [target_tokens_ids[l] for l in examples[\"label\"]]\n",
        "  pattern_examples = [\n",
        "          create_few_shot_examples(examples, 3) +\n",
        "          PATTERN.format(\n",
        "              text1=examples['premise'][idx],\n",
        "              text2=examples['hypothesis'][idx]) + TARGET_PREFIX\n",
        "          for idx in range(len(examples['premise']))\n",
        "      ]\n",
        "  args = (pattern_examples,)\n",
        "  result = tokenizer(*args, padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Get tokens\n",
        "  result[\"input_tokens\"] = [tokenizer.convert_ids_to_tokens(\n",
        "      ids) for ids in result[\"input_ids\"]]\n",
        "\n",
        "  # Decode input\n",
        "  result[\"input_text\"] = [tokenizer.decode(\n",
        "      ids) for ids in result[\"input_ids\"]]\n",
        "\n",
        "  result[\"label\"] = examples[\"label\"]\n",
        "\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQdrf7Kh9yrr"
      },
      "outputs": [],
      "source": [
        "tokenized_mnli_validation_matched_with_context = mnli['validation_matched'].map(preprocess_function_with_few_shot_context, batched=True)\n",
        "tokenized_mnli_validation_mismatched_with_context = mnli['validation_mismatched'].map(preprocess_function_with_few_shot_context, batched=True)\n",
        "tokenized_mnli_training_with_context = mnli['train'].map(preprocess_function_with_few_shot_context, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbSXIZJa_jbn"
      },
      "outputs": [],
      "source": [
        "mnli['train'].map(preprocess_function_without_context, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gshf4pAWdPEA"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  print('predictions, labels: ', predictions, labels)\n",
        "  return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adxf1mgDvzSz"
      },
      "outputs": [],
      "source": [
        "hans = load_dataset(\"hans\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnyWrkwrxtGv"
      },
      "outputs": [],
      "source": [
        " # change features to reflect the new labels\n",
        "features = hans[\"train\"].features.copy()\n",
        "features[\"label\"] = ClassLabel(num_classes=len(tokenizer))\n",
        "hans = hans.cast(features)  # overwrite old features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bCiRb3Ux1Ri"
      },
      "outputs": [],
      "source": [
        "tokenized_hans_without_context = hans.map(preprocess_function_without_context, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHfzdB0wETW"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Get validation_matched and validation_mismatched datasets\n",
        "validation_matched_dataset_without_context = tokenized_mnli_without_context[\"validation_matched\"]\n",
        "validation_mismatched_dataset_without_context = tokenized_mnli_without_context[\"validation_mismatched\"]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "combined_validation_dataset_without_context = concatenate_datasets([validation_matched_dataset_without_context, validation_mismatched_dataset_without_context])\n",
        "\n",
        "# Now combined_validation_dataset contains both validation_matched and validation_mismatched datasets\n",
        "\n",
        "# Optionally, shuffle the combined dataset\n",
        "combined_validation_dataset_without_context = combined_validation_dataset_without_context.shuffle()\n",
        "\n",
        "\n",
        "# Concatenate the two datasets\n",
        "combined_validation_dataset_with_context = concatenate_datasets([tokenized_mnli_validation_matched_with_context, tokenized_mnli_validation_mismatched_with_context])\n",
        "\n",
        "# Optionally, shuffle the combined dataset\n",
        "combined_validation_dataset_with_context = combined_validation_dataset_with_context.shuffle()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EbOaHehzYYD"
      },
      "source": [
        "**Zero Shot Inference In-Domain **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddv5XyASyKjl"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"zero_shot_mnli_validation\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=combined_validation_dataset_without_context,\n",
        "    tokenizer=tokenizer,\n",
        "    # data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u16dVU-zdHZ1"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens([23248])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZiR5wkSyVE6"
      },
      "source": [
        "**Zero Shot Inference Out-Domain **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9shgquHwyQbO"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    eval_dataset=hans['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Jmbhq6fVsu"
      },
      "source": [
        "Few shot inference (ICL) out of domain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wbs2A9K8e1EE"
      },
      "outputs": [],
      "source": [
        "tokenized_hans_validation_with_context = hans['validation'].map(preprocess_function_with_few_shot_context, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    eval_dataset=tokenized_hans_validation_with_context,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yfdw4IgszA"
      },
      "source": [
        "Few shot inference (ICL) in domain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtSdAW_Nf9Q_"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    eval_dataset=combined_validation_dataset_with_context,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC1CHSuB0ihU"
      },
      "source": [
        "Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCZrMmORhU3O"
      },
      "outputs": [],
      "source": [
        "model_to_ft = OPTForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=50272)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCprzYqg-X-d"
      },
      "source": [
        "KL divergence based context distillation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_kl_divergence(logits_with_context, logits_to_align):\n",
        "    # Compute log softmax for both models\n",
        "    # logits_to_align: (batch_size, seq_length, vocab_size)\n",
        "    # logits_to_align: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "    log_probs_to_align = F.log_softmax(logits_to_align, dim=-1)\n",
        "    log_probs_with_context = F.log_softmax(logits_with_context, dim=-1)\n",
        "\n",
        "    # Sort log probabilities and select top 50 indices for each token\n",
        "    _, top_indices = torch.topk(log_probs_with_context, k=50, dim=-1)\n",
        "\n",
        "    # Gather log probabilities of top indices\n",
        "    gathered_log_probs_to_align = torch.gather(log_probs_to_align, -1, top_indices)\n",
        "    gathered_log_probs_with_context = torch.gather(log_probs_with_context, -1, top_indices)\n",
        "\n",
        "\n",
        "    # Compute KL divergence\n",
        "    kl_loss = F.kl_div(gathered_log_probs_with_context, gathered_log_probs_to_align, dim=-1, reduction='batchmean', log_target = True)\n",
        "\n",
        "    return kl_loss"
      ],
      "metadata": {
        "id": "4czj3YTZ7vZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 180\n",
        "\n",
        "# Pad or truncate input sequences to match the model's input length requirement\n",
        "input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) if len(ids) < max_length else ids[:max_length] for ids in tokenized_mnli_training_with_context['input_ids']]\n"
      ],
      "metadata": {
        "id": "PSlJxYA8E9ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_with_context = model(input_ids = torch.tensor(input_ids)).logits"
      ],
      "metadata": {
        "id": "ILU5F33z73Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-Q5IZM-_IXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, OPTForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "    logits_to_align = model(input_ids=inputs['input_ids'])\n",
        "    loss = calculate_kl_divergence(logits_with_context, logits_to_align)\n",
        "    return (loss, logits_to_align) if return_outputs else loss"
      ],
      "metadata": {
        "id": "D9lVaNpV9AkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(task_type= TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
        "\n",
        "\n",
        "peft_model = get_peft_model(model_to_ft, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=\"suryansh/dl-project/mt0-large-lora-context-distillation\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    eval_steps=300,\n",
        "    save_steps=7915,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "context_distillation_trainer = CustomTrainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_mnli_without_context,\n",
        "    eval_dataset=combined_validation_dataset_without_context,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "context_distillation_trainer.train()"
      ],
      "metadata": {
        "id": "IQFORVwv_LmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_distillation_trainer.evaluate()"
      ],
      "metadata": {
        "id": "lM5KOMLLKGka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmbfWJfAnYmv"
      },
      "source": [
        "\n",
        "For each, calculate in-domain accuracy (on mnli) and out-domain accuracy (on hans)\n",
        "\n",
        "1) Try out acuracy for zero shot inference without fine-tuning      \n",
        "2) Try out accuracy for few shot inference (in-context learning) without fine-tuning      \n",
        "3) Try out LoRA, QLoRA and PEFT based fine-tuning      \n",
        "4) KL divergence loss based fine-tuning using LoRA/QLoRa, PEFT      "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTvUmb36FgiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w04MZ56SGRm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}